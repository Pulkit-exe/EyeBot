{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:54:07.396843Z","iopub.status.busy":"2024-07-21T17:54:07.396436Z","iopub.status.idle":"2024-07-21T17:54:22.904908Z","shell.execute_reply":"2024-07-21T17:54:22.903779Z","shell.execute_reply.started":"2024-07-21T17:54:07.396811Z"},"trusted":true},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:54:26.585040Z","iopub.status.busy":"2024-07-21T17:54:26.584585Z","iopub.status.idle":"2024-07-21T17:54:37.391571Z","shell.execute_reply":"2024-07-21T17:54:37.390569Z","shell.execute_reply.started":"2024-07-21T17:54:26.585003Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import shutil\n","from datasets import load_from_disk\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:54:41.621411Z","iopub.status.busy":"2024-07-21T17:54:41.620899Z","iopub.status.idle":"2024-07-21T17:54:41.632211Z","shell.execute_reply":"2024-07-21T17:54:41.630845Z","shell.execute_reply.started":"2024-07-21T17:54:41.621378Z"},"trusted":true},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","\n","# Fine-tuned model name\n","new_model = \"/kaggle/working/Llama-2-7b-chat-finetune\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:55:21.364939Z","iopub.status.busy":"2024-07-21T17:55:21.363947Z","iopub.status.idle":"2024-07-21T17:55:21.391776Z","shell.execute_reply":"2024-07-21T17:55:21.390949Z","shell.execute_reply.started":"2024-07-21T17:55:21.364890Z"},"trusted":true},"outputs":[],"source":["source_dir = \"/kaggle/input/my-data/transformed_dataset\"\n","\n","# Destination directory\n","destination_dir = \"/kaggle/working/transformed_dataset\"\n","\n","shutil.copytree(source_dir, destination_dir)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:56:01.609831Z","iopub.status.busy":"2024-07-21T17:56:01.608991Z","iopub.status.idle":"2024-07-21T17:56:01.614660Z","shell.execute_reply":"2024-07-21T17:56:01.613442Z","shell.execute_reply.started":"2024-07-21T17:56:01.609794Z"},"trusted":true},"outputs":[],"source":["dataset_name = \"/kaggle/working/transformed_dataset\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:56:03.527800Z","iopub.status.busy":"2024-07-21T17:56:03.526883Z","iopub.status.idle":"2024-07-21T17:56:03.553853Z","shell.execute_reply":"2024-07-21T17:56:03.552789Z","shell.execute_reply.started":"2024-07-21T17:56:03.527764Z"},"trusted":true},"outputs":[],"source":["dataset = load_from_disk(dataset_name)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T17:56:08.298049Z","iopub.status.busy":"2024-07-21T17:56:08.297318Z","iopub.status.idle":"2024-07-21T18:08:59.286714Z","shell.execute_reply":"2024-07-21T18:08:59.285643Z","shell.execute_reply.started":"2024-07-21T17:56:08.298014Z"},"trusted":true},"outputs":[],"source":["# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T18:09:14.917904Z","iopub.status.busy":"2024-07-21T18:09:14.916595Z","iopub.status.idle":"2024-07-21T18:09:15.142463Z","shell.execute_reply":"2024-07-21T18:09:15.141498Z","shell.execute_reply.started":"2024-07-21T18:09:14.917865Z"},"trusted":true},"outputs":[],"source":["# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T18:26:54.452992Z","iopub.status.busy":"2024-07-21T18:26:54.452206Z","iopub.status.idle":"2024-07-21T18:27:37.605291Z","shell.execute_reply":"2024-07-21T18:27:37.604191Z","shell.execute_reply.started":"2024-07-21T18:26:54.452959Z"},"trusted":true},"outputs":[],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"Explain the different treatments of glaucoma\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'].split('[/INST]')[1].split('1')[0].strip())"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5424731,"sourceId":9004625,"sourceType":"datasetVersion"},{"modelId":735,"modelInstanceId":3093,"sourceId":4298,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
